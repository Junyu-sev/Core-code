# 使用UKB数据进行PRS

## 一些参考

推荐在进行GWAS和PRS前或进行中遇到问题时，浏览下面的网站，加深对每个环节的理解。

1. **GWAS和PRS教程**（包含QC、PCA的过程）

- [GWAS的参考教程1](https://cloufield.github.io/GWASTutorial/)
- [GWAS的参考教程2](https://plink.readthedocs.io/en/latest/GWAS/)
- [GWAS结果可视化参考教程](https://cloufield.github.io/gwaslab/Visualization/)
- [PRS的参考教程](https://choishingwan.github.io/PRS-Tutorial/)
- [ldsc计算遗传力的参考教程](https://github.com/bulik/ldsc/wiki/Heritability-and-Genetic-Correlation)
- [PRSice-2计算PRS](https://choishingwan.github.io/PRSice/)
- [SCT计算PRS](https://privefl.github.io/bigsnpr/articles/SCT.html)
- [LDpred2、lassosum2计算PRS](https://privefl.github.io/bigsnpr/index.html)
- [lassosum计算PRS](https://github.com/tshmak/lassosum#lassosum-)

2. **plink输出格式**（查询特定后缀文件的列名是什么含义）

- [plink1.9输出文件的格式](https://www.cog-genomics.org/plink/1.9/formats)
- [plink2输出文件的格式](https://www.cog-genomics.org/plink/2.0/formats)

3. **UKB中个体Genotype数据采集、插补及QC过程**

- [Genome-wide genetic data on ~500,000 UK Biobank participants](https://www.biorxiv.org/content/10.1101/166298v1)
   UKB插补时进行了初步的QC，包括MAF<0.0001，缺失超过5%等等，我们在进行GWAS、PRS时可以进一步进行QC。

## 现有数据

1. **UKB个体Genotype插补数据：**`/share/home/longxinyang/bio/2023_ukb/imputation/`
   插补数据中的性别采用的是基因判别的性别（Genetic sex），并非自我报告的性别，因此我认为质控过程不需要check sex。
2. **基因型插补质量INFO：**`/share/home/zhangjunyu/Rawdata/UKB_GWAS/Imputation_INFO/`
   在每份数据中，第8列是INFO分数。
3. **UKB基因组协变量文件：**`/share/home/zhangjunyu/Rawdata/UKB_GWAS/GWAS_cov.csv`
    包括基因型检测的batch、plate、well，基因判别的性别，kinship亲属关系，基因判别的人种，异常值人员，和40个PCA主成分的数据。
4. **UKB计算基因型PCA采用的个体清单：**`/share/home/zhangjunyu/Rawdata/UKB_GWAS/22020_sample_use_in_PCA.txt`
    UKB挑选了407,219个没有亲属、数据缺失少的个体进行PCA分析。注意，UKB做PCA的时候没有筛选人种，它的PCA结果能揭示人种的异质性。

---

## 使用HPC进行PRS
需要两份文件，一份GWAS summary statistics作为Base data，提供候选SNP和β/OR值，另一份个体层面基因型数据作为Target data，进行PRS。
**注意：基础数据和目标数据之间需要没有样本重叠，而且两份数据需要利用同一build构建**。样本重叠可能导致 PRS 与目标数据中测试的性状之间的关联大幅膨胀。膨胀水平与目标样本与基础样本重叠的分数成正比，使用大型基础数据集并不能解决问题（[PMID: 32709988](https://www.nature.com/articles/s41596-020-0353-1)）。

### 设置PRS工作路径

```sh
#创建并设置自己的工作路径
PRSdir="/share/home/zhangjunyu/Project/240814_IHD_PRS"
cd ${PRSdir}
```

### Base data的质量控制

Base data需要以下几列的内容，A1是效应基因；OR是对二元分类变量表型而言的，对于连续变量表型这列应该是BETA；N是样本量sample size。**列名可以按下列表格来设定，或者在运行后续程序时，手动根据各包的要求进行调整**。

|SNP         |CHR |     POS|A1 |A2 |       EAF|     SE|       P|        OR|      N|
|:-----------|:---|-------:|:--|:--|---------:|------:|-------:|---------:|------:|
|rs2691328   |1   |   13668|A  |G  | 0.0060030| 0.0818| 0.90861| 1.0094443| 412181|
|rs185038034 |1   |  890252|G  |A  | 0.0001641| 0.3045| 0.75289| 0.9085549| 412181|
|rs76759405  |1   | 1087428|T  |C  | 0.0006499| 0.1325| 0.19628| 0.8426530| 412181|
|rs374733505 |1   | 1268202|A  |G  | 0.0003528| 0.1879| 0.63372| 0.9143883| 412181|
|rs907392344 |1   | 1445960|G  |C  | 0.0002067| 0.2845| 0.63924| 0.8751150| 412181|
|rs983186821 |1   | 1662225|C  |A  | 0.0005477| 0.1542| 0.62783| 1.0776686| 412181|

1. **估计遗传力，需要base data的h<sup>2</sup><sub>SNP</sub> > 0.05**

    ```sh
    conda activate /share/home/zhangjunyu/anaconda3/envs/ldsc
    mkdir -p "./Base_data"
    #将base data放入 ./Base_data 文件夹，并修改base_data的路径
    base_data="./Base_data/base_data.tsv.gz"
    ```

    利用ldsc进行遗传力估计。实测这一步非常耗时，我没有找到用这个脚本并行计算的方法。酌情，不行就不做遗传力检验了。

    ```sh
    mkdir -p "./Base_data/ldsc_result"
    /share/home/zhangjunyu/Software/ldsc/munge_sumstats.py \
        --sumstats ${base_data}\
        --out ./Base_data/ldsc_result/ldsc_result \
        --merge-alleles /share/home/zhangjunyu/Software/ldsc/w_hm3.snplist

    ldsc.py \
        --h2 ./Base_data/ldsc_result/ldsc_result.sumstats.gz \
        --ref-ld-chr /share/home/zhangjunyu/Software/ldsc/eur_w_ld_chr/ \
        --w-ld-chr /share/home/zhangjunyu/Software/ldsc/eur_w_ld_chr/ \
        --out ./Base_data/ldsc_result/scz_h2
    ```

    生成的`scz_h2.log`文件中该份GWAS数据SNP遗传力的大小(括号里面是SE)见这一行:`Total Observed scale h2: 0.0479 (0.0028)`

2. **去除重复的SNP，rsid为NA的SNP，MAF<1%的SNP，模棱两可的 SNP**

    在终端激活一个有data.table和tidyverse的环境，进入R编译器:

    ```sh
    conda activate /share/home/zhangjunyu/anaconda3/envs/PRS
    R
    ```

    利用R进行QC:

    ```R

    #在这里修改base_data的路径
    base_data="./Base_data/base_data.tsv.gz"
    library(data.table)
    library(tidyverse)
    dat <- fread(base_data)
    #去掉重复SNP
    dat <- dat[!duplicated(dat$SNP),]
    #去除rsid为NA的snp
    dat <- dat %>% filter(!is.na(SNP))
    #仅保留EAF（MAF）> 0.01的SNP
    dat <- dat %>% filter(EAF > 0.01)
    #去除A1A2互补的SNP，这样的SNP在DNA测序中可能识别错误
    complement <- function(x) {
    switch (
        x,
        "A" = "T",
        "C" = "G",
        "T" = "A",
        "G" = "C",
        return(NA)
    )}
    dat <- dat %>% rowwise() %>% filter(A2 != complement(A1))
    #输出完成QC文件
    fwrite(dat,"./Base_data/Base_data_after_QC", row.names=F, quote=F, sep= "\t")
    q()
    ```

    在终端进行数据压缩，生成.gz文件

    ```sh
    gzip ./Base_data/Base_data_after_QC
    ```

### Target data的质量控制与PCA

Target data（个体层面基因型数据）的数据提取、质量控制以及PCA过程和进行GWAS时是一致的。

0. **待提取个体文件**
    将进行**PRS**的个体编号总结在一份.txt文件中，需要两列编号，列名为"FID"和"IID"，格式如下：

    ```sh
    FID	IID
    1000619	1000619
    1003210	1003210
    1004545	1004545
    1004716	1004716
    1005251	1005251
    1007707	1007707
    1008517	1008517
    1009506	1009506
    1009521	1009521
    ```

    **设置提取个体文件的路径：**

    ```sh
    #举一个例子，下面这个all_IID.txt是所有白人，要替换成自己的文件
    Inclusion="/share/home/zhangjunyu/Project/240721_IHD_GWAS/Data/Inclusion/All_IID.txt"
    ```

1. **创建工作路径，进入工作路径**

    ```sh
    workdir=${PRSdir}/"Target_data"
    cd ${workdir}
    ```

2. **提取进行GWAS分析的个体**
    通过代码自动生成并提交LSF任务。提取目标个体，将每个染色体bgen原始数据转换为plink2的pgen格式。这是在新阳师姐代码基础上改动的，还没有得到师姐授权（小声）。

    ```sh
    #!/bin/bash
    #创建目标文件夹
    Filefold1="01.Extract_participants"
    mkdir -p ${Filefold1}
    #定义所有染色体标记，开始循环
    chromosomes=( {1..22} X XY )

    for chr in "${chromosomes[@]}"; do
        #生成脚本文件名
        file1="${Filefold1}/chr${chr}_subsample.sh"
        #写入内容到脚本文件
        cat <<EOF > "$file1"
    #!/bin/bash
    #BSUB -J chr$chr
    #BSUB -n 40
    #BSUB -R "span[ptile=40]"
    #BSUB -o output_%J
    #BSUB -e output_%J
    #BSUB -q mpi

    cd ${workdir}

    # 将bgen转换为pgen，并挑选目标人群
    genotypeFile="/share/home/longxinyang/bio/2023_ukb/imputation/ukb22828_c${chr}_b0_v3.bgen"
    sample="/share/home/longxinyang/bio/2023_ukb/imputation/ukb22828_c${chr}_b0_v3.sample"

    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
    --memory 409600 --threads 35 \
    --bgen "\${genotypeFile}" 'ref-first' --sample "\${sample}" \
    --snps-only \
    --keep "${Inclusion}" \
    --rm-dup 'force-first' \
    --make-pgen --out "./${Filefold1}/ukb22828_c${chr}_subsample"
    
    EOF

        # 为生成的脚本文件设置执行权限
        chmod +x "$file1"
        # 提交任务
        bsub < "$file1"
    done

    echo "所有脚本已生成并完成提交。"
    ```

3. **合并基因型数据**
    将提取得到的1-22号染色体数据合并，生成plink格式的.bed文件。这个也是在新阳师姐代码基础上改动的，还没有得到师姐授权（小声）。

    ```sh
    #创建目标文件夹
    Filefold2="02.Merge_chromosome"
    mkdir -p ${Filefold2}
    #生成目标脚本
    file2="02.merge_chr1_22.sh"
    cat <<EOF > "$file2"
    #!/bin/bash
    #BSUB -J GWAS_merge
    #BSUB -n 40
    #BSUB -R "span[ptile=40]"
    #BSUB -o output_%J
    #BSUB -o output_%J
    #BSUB -q mpi

    cd ${workdir}

    # 生成pgen.list记录要合并的文件的前缀
    output_file="pgen.list"
    chromosomes=( {1..22}  )

    # chromosomes=(19 21)
    > "\$output_file"
    for chr in "\${chromosomes[@]}"; do
        echo "ukb22828_c\${chr}_subsample" >> "\$output_file"
    done

    # 合并所有常染色体的pgen，生成.bed文件
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
    --memory 2000000 --threads 40 \
    --pmerge-list-dir ./01.Extract_participants \
    --pmerge-list pgen.list \
    --make-bed \
    --out ./${Filefold2}/merge
    EOF

    # 为生成的脚本文件设置执行权限
    chmod +x "$file2"
    # 提交任务
    bsub < "$file2"
    ```

4. **进行GWAS质控**

    ```sh
    #创建目标文件夹
    Filefold3="03.GWAS_QC"
    mkdir -p ${Filefold3}
    #生成目标脚本
    file3="03.GWAS_QC.sh"
    cat <<EOF > "$file3"
    #!/bin/bash
    #BSUB -J GWAS_QC
    #BSUB -n 40
    #BSUB -R "span[ptile=40]"
    #BSUB -o output_%J
    #BSUB -o output_%J
    #BSUB -q mpi

    cd ${workdir}
    
    #根据基因频率、缺失情况、hwe质控，这里的阈值都可以自行调整
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./02.Merge_chromosome/merge \
        --maf 0.01 \
        --geno 0.02 \
        --mind 0.02 \
        --hwe 1e-6 \
        --make-bed \
        --out ./${Filefold3}/QC1

    #计算F系数筛掉杂合性异常的样本，计算前先进行pruning
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./${Filefold3}/QC1 \
        --indep-pairwise 50 5 0.2 \
        --out ./${Filefold3}/QC1

    #计算F系数筛掉杂合性异常的样本
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./${Filefold3}/QC1 \
        --extract ./${Filefold3}/QC1.prune.in \
        --het \
        --out ./${Filefold3}/QC2
    
    #用R筛选F系数属于mean±3SD之外的个体
    /share/home/zhangjunyu/anaconda3/envs/PRS/bin/Rscript - <<EOL
    library(data.table)
    library(tidyverse)
    het <- fread('03.GWAS_QC/QC2.het')
    mean_F <- mean(het\$F, na.rm=T)
    sd_F <- sd(het\$F, na.rm = TRUE)
    sample <- het %>% filter(F > mean_F + 3*sd_F | F < mean_F - 3*sd_F)
    sample <- sample[,1:2]
    fwrite(sample,'03.GWAS_QC/QC2.sample',row.names=F,col.names=F,sep='\t')
    EOL

    #筛除F系数属于mean±3SD之外的个体
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./${Filefold3}/QC1 \
        --remove ./${Filefold3}/QC2.sample \
        --keep-allele-order \
        --make-bed \
        --out ./${Filefold3}/QC2

    #去除一级亲属关系（阈值kinship=0.177）的个体
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./${Filefold3}/QC2 \
        --king-cutoff 0.177 \
        --make-bed \
        --out ./${Filefold3}/QC3    
    EOF

    # 为生成的脚本文件设置执行权限
    chmod +x "$file3"
    # 提交任务
    bsub < "$file3"
    ```

5. **PCA及协变量文件构建**

    选择适当数量的 PC 的一种方法是使用不同数量的 PC 对所研究的表型执行 GWAS，然后可以对 GWAS 汇总统计量集进行 LDSC 分析，最接近 1 的 LDSC 截距的 PC 数量的 GWAS 应对应于最精确控制群体结构的 PC。[（PRS的参考教程）](https://choishingwan.github.io/PRS-Tutorial/)我个人感觉大部分研究或教程选用的是10个主成分，在这里从简，选择计算10个主成分。

    ```sh
    # 创建目标文件夹
    Filefold4="04.PCA"
    mkdir -p ${Filefold4}
    #生成目标脚本
    file4="04.GWAS_PCA.sh"
    cat <<EOF > "$file4"
    #!/bin/bash
    #BSUB -J GWAS_QC
    #BSUB -n 40
    #BSUB -R "span[ptile=40]"
    #BSUB -o output_%J
    #BSUB -o output_%J
    #BSUB -q mpi

    cd ${workdir}
    #去除high-LD或HLA区域的SNP
    #因为UKB使用的是hg19版本的基因，所以我们使用了high-ld-hg19.txt，如果是hg38版本，请使用high-ld-hg38.txt
    /share/home/zhangjunyu/Software/plink/plink/plink \
        --bfile ./03.GWAS_QC/QC3 \
        --make-set /share/home/zhangjunyu/Rawdata/UKB_GWAS/high-ld-hg19.txt \
        --write-set \
        --out ./${Filefold4}/hild
    
    #进一步进行pruning，挑选合适的SNP
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./03.GWAS_QC/QC3 \
        --maf 0.01 \
        --exclude ./${Filefold4}/hild.set \
        --indep-pairwise 500 50 0.2 \
        --out ./${Filefold4}/PCA

    #挑选进行PCA的个体，去除相关联的个体（阈值kinship=0.0884）
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./03.GWAS_QC/QC3 \
        --extract ./${Filefold4}/PCA.prune.in \
        --king-cutoff 0.0884 \
        --out ./${Filefold4}/PCA

    #对挑选后的SNP以及个体进行PCA
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./03.GWAS_QC/QC3 \
        --keep ./${Filefold4}/PCA.king.cutoff.in.id \
        --extract ./${Filefold4}/PCA.prune.in \
        --freq counts \
        --pca approx allele-wts 10 \
        --out ./${Filefold4}/PCA
    
    #将PCA模型映射到所有数据，获取每个个体的PCA主成分取值
    /share/home/zhangjunyu/Software/plink/plink2/plink2 \
        --memory 2000000 --threads 40 \
        --bfile ./03.GWAS_QC/QC3 \
        --read-freq ./${Filefold4}/PCA.acount \
        --score ./${Filefold4}/PCA.eigenvec.allele 2 6 header-read no-mean-imputation variance-standardize \
        --score-col-nums 7-16 \
        --out ./${Filefold4}/PCA_projected
    EOF

    # 为生成的脚本文件设置执行权限
    chmod +x "$file4"
    # 提交任务
    bsub < "$file4"
    ```

    构建GWAS协变量文件。在这里，选取性别、年龄、PCA的前十主成分作为协变量。

    ```R
    library(data.table)
    library(tidyverse)
    covariate <- fread("/share/home/zhangjunyu/Rawdata/UKB_GWAS/GWAS_PCA.csv", select=c("eid", "Genetic_sex", "age"))
    covariate <- covariate %>% rename(IID="eid")
    column_names <- fread("./04.PCA/PCA_projected.sscore", nrows = 10) %>% names()
    column_names <- grep("IID|^PC", column_names, value = TRUE)
    PCA <- fread("./04.PCA/PCA_projected.sscore",select=column_names)
    covariate_merge <- right_join(covariate, PCA, by = "IID")
    covariate_merge <- covariate_merge %>% mutate(FID=IID)
    covariate_merge <- covariate_merge[,c(14,1:13)]
    fwrite(covariate_merge,"GAWS_cov.txt", row.names=F, quote=F, sep="\t")
    ```

6. **表型文件**
   整理表型数据到一份.txt文件中。表型文件固定前两列为个体编号，列名为"FID"和"IID"，第三列或其他列为表型。
   - 连续变量作为表型，格式如下：

    ```sh
    FID	IID	pheno
    1000619	1000619	56.1603
    1003210	1003210	76.1989
    1004545	1004545	69.6949
    1004716	1004716	66.5986
    1005251	1005251	58.7845
    1007707	1007707	55.6963
    1008517	1008517	58.7492
    1009506	1009506	23.5583
    1009521	1009521	66.5572
    ```

   - 二分类变量作为表型，**注意：二分类性状中1表示control，2表示case**，格式如下：

    ```sh
    FID	IID	pheno
    1000038	1000038	1
    1000093	1000093	1
    1000108	1000108	2
    1000115	1000115	1
    1000151	1000151	1
    1000166	1000166	2
    1000173	1000173	1
    1000182	1000182	1
    1000201	1000201	1
    ```

    **设置表型文件的路径：**

    ```sh
    #举一个例子，下面这个all_IID.txt是是否患IHD的二元分类性状，要替换成自己的文件
    Pheno="/share/home/zhangjunyu/Project/240721_IHD_GWAS/Data/Phenotype/All_IHD.txt"
    ```

7. **修改.fam文件，将表型数据加入到其中第6列**
    这一步是因为在计算SCT-PRS、LDpred2-grid评分的时候，需要表型置于fam文件中，待修改的文件是这个：
    `echo ${workdir}/03.GWAS_QC/QC3.fam`
    只需要把之前生成的pheno文件中的表型合并到fam文件的最后一列就可以。**再次注意：二元分类变量中，1表示control，2表示case**

### PRS计算

PRS计算的四种方法包括：**C+T方法、Stacked C+T方法、基于贝叶斯的LDpred2方法以及基于惩罚回归的Lassosum方法**。

首先进入我计算PRS的环境。

```sh
conda activate /share/home/zhangjunyu/anaconda3/envs/PRS
cd ${PRSdir}
```

#### C+T方法（PRSice-2计算）

CT方法是最传统的方法，在进行PRS的计算时，有clumping的过程。**通过固定clumping的kb和r2选项，用不同的阈值进行计算，挑选最优的P值阈值**，然后在最优阈值下计算每个个体的PRS评分。

需要先复制文件夹`/share/home/zhangjunyu/Rawdata/PRSice2/lib`到工作路径中，或者在login节点操作，能自动下载。

```sh
#创建并进入路径
mkdir -p CT
cd CT

#如果需要复制
cp -r /share/home/zhangjunyu/Rawdata/PRSice2/lib ./CT

#PRSice-2计算PRS(对于连续变量)
/share/home/zhangjunyu/anaconda3/envs/PRS/bin/Rscript /share/home/zhangjunyu/Software/PRSice/PRSice.R \
    --prsice /share/home/zhangjunyu/Software/PRSice/PRSice_linux \
    --base ${PRSdir}/Base_data/Base_data_after_QC.gz \
    --bp POS \
    --target ${PRSdir}/Target_data/03.GWAS_QC/QC3 \
    --binary-target F \
    --pheno ${Pheno} \
    --cov ${PRSdir}/Target_data/GAWS_cov.txt \
    --base-maf MAF:0.01 \
    --base-info INFO:0.8 \
    --stat BETA \
    --out ${PRSdir}/CT/PRSice2_result

#PRSice-2计算PRS(对于二元分类变量)
/share/home/zhangjunyu/anaconda3/envs/PRS/bin/Rscript /share/home/zhangjunyu/Software/PRSice/PRSice.R \
    --prsice /share/home/zhangjunyu/Software/PRSice/PRSice_linux \
    --base ${PRSdir}/Base_data/Base_data_after_QC.gz \
    --bp POS \
    --target ${PRSdir}/Target_data/03.GWAS_QC/QC3 \
    --binary-target T \
    --pheno ${Pheno} \
    --cov ${PRSdir}/Target_data/GAWS_cov.txt \
    --base-maf MAF:0.01 \
    --base-info INFO:0.8 \
    --stat OR \
    --or \
    --out ${PRSdir}/CT/PRSice2_result

cd ${PRSdir}
```

运行结束后，产生:

- **.best文件**：最佳P阈值下，每个个体计算得到的PRS评分；
- **.summary文件**：最佳拟合模型的信息，Threshold是最佳P值阈值，PRS.R2是PRS解释的差异，Full.R2是由完整模型（包括协变量）解释的方差;
- **.prsice文件**：PRS不同阈值拟合的数值结果；
- 两份 **.png文件**：显示了在所有P值阈值下计算的PRS的拟合效果，对prsice文件的可视化；

#### SCT方法

SCT方法是C+T方法的改进，认为**clumping的kb和r2选项和插补质量INFO同样可以变动**，在所有的C+T参数组合中多次计算，综合得出每个个体最终的多基因风险评分。有以下特色：

1. 使用交叉验证，将个体级数据划分为验证集(Validation Set)和测试集(Test Set)，在验证集中训练出最优的参数组合，并在测试集中评价其预测效果。
2. 融入机器学习思想，在验证集中，计算所有参数组合的C+T得分，对于**各种组合**的多基因风险评分纳入惩罚回归中训练回归系数以防止潜在的过拟合问题，**加权求和**得到最后的多基因风险评分，并在测试集中评价其预测效果，最终得到最优的参数和PRS评分。

但是，由于我们用PRS预测的时候，会有外部数据集验证，再加上我们现在的核心目标在于计算PRS，所以**后续所有涉及交叉验证的方法，均采用全部数据集做训练集，同时省略测试集验证的过程**。如果需要进行内部验证，可以**修改`ind.row`参数**(参考最上方的教程或者来找我)。

创建SCT的路径，进入R

```sh
mkdir -p SCT
R
```

```R
#读取包
library(bigsnpr)
library(tidyverse)
#有一点点冲突，只有这么设置，并行计算时不会报错
options(bigstatsr.check.parallel.blas = FALSE)
options(default.nproc.blas = NULL)
#设置并行计算核的数目
NCORES <- 30

#读取target data
snp_readBed("./Target_data/QC3.bed/03.GWAS_QC") #只在第一次读取时需要这行代码，生成rds后并不需要再次运行这行代码（而且再次运行时会报错）
obj.bigSNP <- snp_attach("./Target_data/03.GWAS_QC/QC3.rds")

#从target data中提取需要的内容
genotype <- obj.bigSNP$genotypes
#因为G(UKB基因型数据)中含有缺失值，bigsnpr计算不允许缺失值，所以我们需要进行插补，bigsnpr目前提供了snp_fastImputeSimple和snp_fastImpute函数。其中，snp_fastImputeSimple可以用均值、众数来插补，snp_fastImpute使用XGBoost进行插补。在这里采用均值插补做一个例子。
genotype <- snp_fastImputeSimple(G,  method = "mean0",  ncores = 30)
CHR <- obj.bigSNP$map$chromosome
POS <- obj.bigSNP$map$physical.pos
y   <- obj.bigSNP$fam$affection - 1

#读取base data
sumstats <- bigreadr::fread2("./Base_data/Base_data_after_QC.gz", select = c(2,1,3,4,5,9,8))
str(sumstats)

#如果是二元分类变量，需要转换为beta
sumstats <- sumstats %>% mutate(OR = log(OR))
names(sumstats) <- c("chr", "rsid", "pos", "a0", "a1", "beta", "p")
map <- obj.bigSNP$map[,-(2:3)]
names(map) <- c("chr", "pos", "a0", "a1")

#进行base data和target data的SNP配对
info_snp <- snp_match(sumstats, map)

#提供SNP中beta和-logP信息的向量，没有配对上的保留为NA
beta <- rep(NA, ncol(genotype))
beta[info_snp$`_NUM_ID_`] <- info_snp$beta
lpval <- rep(NA, ncol(genotype))
lpval[info_snp$`_NUM_ID_`] <- -log10(info_snp$p)

#进行clumping，这一步会选用多种kb和R2的组合分别进行clumping，选用的组合保存在attr里面
ind.train <- 1:nrow(genotype)
all_keep <- snp_grid_clumping(genotype, CHR, POS, ind.row = ind.train,
                              lpS = lpval, exclude = which(is.na(lpval)),
                              ncores = NCORES)

#对于不同参数的阈值，计算PRS
##先创建一个输出backingfile的路径
dir.create("./SCT/tmp-data", showWarnings = FALSE)
multi_PRS <- snp_grid_PRS(genotype, all_keep, beta, lpval, ind.row = ind.train,
                          backingfile = "./SCT/tmp-data/data-scores", 
                          n_thr_lpS = 50, ncores = NCORES)

#使用惩罚回归来学习最优线性 C+T 分数的组合
ind.train <- 1:nrow(genotype)
final_mod <- snp_grid_stacking(multi_PRS, y[ind.train], ncores = NCORES)
summary(final_mod$mod)
#SCT方法的精髓在于stack，在 stacking 过程中，通过最大化模型拟合效果，会为每个 C+T 模型分配一个权重，然后综合计算每个SNP的效应值（来替换SNP的β作为权重）。
new_beta <- final_mod$beta.G
ind <- which(new_beta != 0)

#使用这个变异权重向量来计算多基因风险，这里的pred就是计算得到的每个个体的PRS评分，可以输出
ind.test <- 1:nrow(genotype)
pred_SCT <- final_mod$intercept + 
  big_prodVec(genotype, new_beta[ind], ind.row = ind.test, ind.col = ind)

#输出文件
result <- tibble(obj.bigSNP$fam$sample.ID,pred_SCT)
names(result) <- c("eid", "SCT_prs")
fwrite(result, "./SCT/SCT_prs.txt", sep="\t", row.names=F, quote=F)
```

#### LDPred2方法

**LDpred2 是一种用于基因型数据的贝叶斯混合模型，它可以利用连锁不平衡（LD）信息来对全基因组关联研究（GWAS）中的效应量进行更精确的估计**。LDpred2方法有三种计算模式，包括LDpred2-inf、LDpred2(-grid)和LDpred2-auto。三种模式的区别请询问Chat-gpt。

在分析前需要copy需要的数据到当前路径，并进入R编译器：

```sh
cd ${PRSdir}
mkdir -p "./LDpred2/tmp-data"
cp  -r /share/home/zhangjunyu/Rawdata/LDpred2/* ./LDpred2/tmp-data
R
```

```R
#读取包
library(bigsnpr)
library(data.table)
library(tidyverse)
#有一点点冲突，只有这么设置，并行计算时不会报错
options(bigstatsr.check.parallel.blas = FALSE)
options(default.nproc.blas = NULL)
#设置并行计算核的数目
NCORES <- 30
#设置随机数种子
set.seed(2024)

#读取HapMap3+变体，后续分析只保留HapMap3+变体
info <- readRDS("./LDpred2/tmp-data/map_hm3_plus.rds")

#读取base data和数据标准化
sumstats <- bigreadr::fread2("./Base_data/Base_data_after_QC.gz") 
##不论连续变量表型，还是二分类变量表型，sumstats都需要变成以下形式（包括列名）
##rsid, chr, pos, a1, a0, MAF, beta_se, p, beta, n_eff
##请根据自己的base data进行数据调整，以下只是一个示例
names(sumstats) <- c("rsid", "chr", "pos", "a1", "a0", "MAF", "beta_se", "p", "OR", "n_eff")
sumstats <- sumstats %>% mutate(chr=as.integer(chr))
sumstats <- sumstats %>% mutate(OR = log(OR))
sumstats <- sumstats %>% rename(beta="OR")
#只保留HapMap3+变体
sumstats <- sumstats[sumstats$rsid %in% info$rsid,]

#读取target data
##只在第一次读取时需要这行代码，生成rds后并不需要再次运行这行代码（而且再次运行时会报错）
snp_readBed("./Target_data/03.GWAS_QC/QC3.bed")
##生成rds文件后，只需要直接读取rds文件
obj.bigSNP <- snp_attach("./Target_data/03.GWAS_QC/QC3.rds")
#将target data的各种信息分别储存
genotype <- obj.bigSNP$genotypes
CHR <- obj.bigSNP$map$chromosome
POS <- obj.bigSNP$map$physical.pos
y   <- obj.bigSNP$fam$affection - 1
#因为genotype(UKB基因型数据)中含有缺失值，所以我们需要进行插补，bigsnpr目前提供了snp_fastImputeSimple和snp_fastImpute函数。其中，snp_fastImputeSimple可以用均值、众数来插补，snp_fastImpute使用XGBoost进行插补。在这里采用均值插补做一个例子。
genotype <- snp_fastImputeSimple(genotype,  method = "mean0",  ncores = 30)

#base data和target data进行配对
map <- obj.bigSNP$map[-3]
names(map) <- c("chr", "rsid", "pos", "a1", "a0")
info_snp <- snp_match(sumstats, map)

#将physical positions（bp单位）转变为genetic positions (cM单位)
POS2 <- snp_asGeneticPos(CHR, POS, dir = "./LDpred2/tmp-data")

#在磁盘上创建稀疏全基因组相关矩阵
##这步会生成一个临时.sbk文件，但是不会主动删除。再次运行snp_asGeneticPos()时，.sbk文件存在会报错，需要手动删除.sbk文件
tmp <- tempfile(tmpdir = "./LDpred2/tmp-data")
for (chr in 1:22) {
    #提取当前染色体上的SNP
    ind.chr <- which(info_snp$chr == chr)
    ind.chr2 <- info_snp$`_NUM_ID_`[ind.chr]
    # Calculate the LD
    corr0 <- snp_cor(genotype, ind.col = ind.chr2, ncores = NCORES,
            infos.pos = POS2[ind.chr2], size = 3 / 1000)
    if (chr == 1) {
        ld <- Matrix::colSums(corr0^2)
        corr <- as_SFBM(corr0, tmp)
    } else {
        ld <- c(ld, Matrix::colSums(corr0^2))
        corr$add_columns(corr0, nrow(corr))
    }
}

#通过ldsc来计算遗传力
df_beta <- info_snp[,c("beta", "beta_se", "n_eff", "_NUM_ID_")]
ldsc <- snp_ldsc(   ld, 
                    length(ld), 
                    chi2 = (df_beta$beta / df_beta$beta_se)^2,
                    sample_size = df_beta$n_eff, 
                    blocks = NULL)
h2_est <- ldsc[["h2"]]
```

接下来，按照LDpred2的三种类型，分别计算PRS。尽管官网教程中提及"LDpred2-inf的性能很可能比其他模型差,建议不要再使用它"，但是我依旧保留了LDpred2-inf的计算过程。

1. **LDpred2-inf模式（infinitesimal model）**

```R
#获取LDpred2-inf模式计算的SNP权重
beta_inf <- snp_ldpred2_inf(corr, df_beta, h2 = h2_est)

#计算每个个体的PRS评分
ind.test <- 1:nrow(genotype)
pred_inf <- big_prodVec(genotype, beta_inf, ind.row = ind.test,
                        ind.col = info_snp$`_NUM_ID_`)

#保存LDpred2-auto PRS评分
result <- tibble(obj.bigSNP$fam$sample.ID,pred_inf)
names(result) <- c("eid", "LDpred2_inf_prs")
fwrite(result, "./LDpred2/LDpred2_inf_prs.txt", sep="\t", row.names=F, quote=F)
```

2. **LDpred2-grid模式（grid of models）**

```R
#设置进行grid的参数范围。grid网格搜索，穷举设定参数的可选数值
p_seq <- signif(seq_log(1e-5, 1, length.out = 21), 2)
h2_seq <- round(h2_est * c(0.3, 0.7, 1, 1.4), 4)
params <- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE))

#获取LDpred2-grid模式计算的SNP权重
beta_grid <- snp_ldpred2_grid(corr, df_beta, params, ncores = NCORES)

#计算每个模型的PRS结果
pred_grid <- big_prodMat(genotype, beta_grid, ind.col = info_snp$`_NUM_ID_`)

#获取最佳参数，使用来自（线性或逻辑）回归的Z-score作为评判的依据
##当然也可以选用AUC或模型r2，作者说他发现Z-score是最好的
##整体上这里的选择有很多操作的空间
ind.val <- 1:nrow(genotype)
params$score <- apply(pred_grid[ind.val, ], 2, function(x) {
  if (all(is.na(x))) return(NA)
  #连续变量选用
  #summary(lm(y[ind.val] ~ x))$coef["x", 3]
  #二元分类变量选用
  summary(glm(y[ind.val] ~ x, family = "binomial"))$coef["x", 3]
})
params %>%
    mutate(sparsity = colMeans(beta_grid == 0), id = row_number()) %>%
    arrange(desc(score)) %>%
    mutate_at(c("score", "sparsity"), round, digits = 3) %>% slice(1:10)
best_beta_grid <- params %>% mutate(id = row_number()) %>%
  # filter(sparse) %>% 
  arrange(desc(score)) %>% slice(1) %>% print() %>% pull(id) %>% beta_grid[, .]
#计算每个个体的最佳PRS评分
ind.test <- 1:nrow(genotype)
pred <- big_prodVec(genotype, best_beta_grid, ind.row = ind.test,
                    ind.col = df_beta[["_NUM_ID_"]])


#保存LDpred2-auto PRS评分 #这里还有问题
result <- tibble(obj.bigSNP$fam$sample.ID,pred)
names(result) <- c("eid", "LDpred2_grid_prs")
fwrite(result, "./LDpred2/LDpred2_grid_prs.txt", sep="\t", row.names=F, quote=F)
```

3. **LDpred2-auto模式（automatic model）**

```R
coef_shrink <- 0.95 # reduce this up to 0.4 if you have some (large) mismatch with the LD ref

#设置参数
multi_auto <- snp_ldpred2_auto(
                corr, df_beta, h2_init = h2_est,
                vec_p_init = seq_log(1e-4, 0.9, length.out = 30),
                ncores = NCORES, allow_jump_sign = FALSE,
                shrink_corr = coef_shrink)

#去除不成功的拟合（偏离特别大的拟合）
range <- sapply(multi_auto, function(auto) diff(range(auto$corr_est)))
keep <- which(range > (0.95 * quantile(range, 0.95, na.rm = TRUE)))

#从拟合中，获取LDpred2-auto模式计算的最佳SNP权重
beta_auto <- rowMeans(sapply(multi_auto[keep], function(auto) auto$beta_est))

#计算每个个体的PRS评分
ind.test <- 1:nrow(genotype)
pred_auto <- big_prodVec(genotype, beta_auto, ind.row = ind.test, ind.col = df_beta[["_NUM_ID_"]])

#保存LDpred2-auto PRS评分
result <- tibble(obj.bigSNP$fam$sample.ID, pred_auto)
names(result) <- c("eid", "LDpred2_auto_prs")
fwrite(result, "./LDpred2/LDpred2_auto_prs.txt", sep="\t", row.names=F, quote=F)
```

#### lassosum方法

**lassosum方法使用参考人群的数据进行LD校正，结合Lasso回归优化SNP权重，进行PRS评分的计算**。

创建路径并进入R：

```sh
mkdir -p "./Lassosum"
R
```

```R
library(lassosum)
library(data.table)
library(methods)
library(tidyverse)
library(parallel)
#使用30核进行计算 
cl <- makeCluster(30)

#读取base_data
sum.stat <- "./Base_data/Base_data_after_QC.gz"
ss <- fread(sum.stat)
#去除base_data中P==0的SNP（因为读取精度问题，小于1E-303的数值会被读为0）
ss <- ss[!P == 0]
#设置target_data的路径
bfile <- "./Target_data/03.GWAS_QC/QC3"
#读取协变量数据
covariate <- fread("./Target_data/GAWS_cov.txt")
covariate <- as.data.frame(covariate)
#读取表型文件，需要替换路径为自己的文件
target.pheno <- fread("/share/home/zhangjunyu/Project/240721_IHD_GWAS/Data/Phenotype/All_IHD.txt")
target.pheno <- target.pheno %>% filter(FID %in% unlist(covariate$FID))

#使用hg19版本的基因数据作为ld的参考序列
ld.file <- "EUR.hg19"

#计算SNP的相关性
cor <- p2cor(p = ss$P, n = ss$N, sign = log(ss$OR))
fam <- fread(paste0(bfile, ".fam"))
fam[,ID:=do.call(paste, c(.SD, sep=":")),.SDcols=c(1:2)]

#运行lassosum pipeline
out <- lassosum.pipeline(
    cor = cor,
    chr = ss$CHR,
    pos = ss$POS,
    A1 = ss$A1,
    A2 = ss$A2,
    ref.bfile = bfile,
    test.bfile = bfile,
    LDblocks = ld.file, 
    cluster=cl
)
#进行验证
target.res <- validate(out, pheno = as.data.frame(target.pheno), covar=as.data.frame(covariate))
#得到最大的R2值是多少

result <- target.res$results.table[,c(1,5)]
names(result) <- c("eid", "Lassosum_prs")
fwrite(result, "./Lassosum/Lassosum_prs.txt", sep="\t", row.names=F, quote=F)
```

Tips：lassosum方法也可以通过计算LDpred2的bigsnpr包实现，请参阅[“LDpred2、lassosum2计算PRS”](https://privefl.github.io/bigsnpr/index.html)。

---
